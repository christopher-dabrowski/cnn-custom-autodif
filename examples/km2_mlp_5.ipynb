{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388e2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Library/CloudStorage/OneDrive-Personal/Documents/Studia/Semestr 8/Algorytmy w inżynierii danych/Projekt/KamienMilowy2/KM2_Piotr_Szczerba`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab75b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (6.10s) \tTrain: (loss: 0.5818, acc: 0.8061)\n",
      "Epoch: 2 (2.25s) \tTrain: (loss: 0.3451, acc: 0.9307)\n",
      "Epoch: 3 (2.95s) \tTrain: (loss: 0.2210, acc: 0.9606)\n",
      "Epoch: 4 (2.37s) \tTrain: (loss: 0.1507, acc: 0.9779)\n",
      "Epoch: 5 (2.28s) \tTrain: (loss: 0.1062, acc: 0.9889)\n"
     ]
    }
   ],
   "source": [
    "include(\"../autodiff/graph.jl\")\n",
    "include(\"../autodiff/forward.jl\")\n",
    "include(\"../autodiff/backward.jl\")\n",
    "include(\"../autodiff/operators.jl\")\n",
    "include(\"../neuralnet/dataloader.jl\")\n",
    "\n",
    "using JLD2\n",
    "using Random\n",
    "using Statistics\n",
    "using Printf\n",
    "\n",
    "X_train = load(\"../data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"../data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "X_test = load(\"../data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test = load(\"../data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "\n",
    "y_train = Float32.(y_train)\n",
    "y_test = Float32.(y_test)\n",
    "\n",
    "dataset = DataLoader(X_train, y_train, 64, shuffle=true)\n",
    "\n",
    "input_neurons  = size(X_train, 1)\n",
    "hidden_neurons = 32\n",
    "output_neurons = 1\n",
    "\n",
    "ϵ = Constant(1e-7)\n",
    "binary_cross_entropy_loss(y, ŷ) = mean(Constant(-1.0) .* (y .* log.(ŷ .+ ϵ) .+ (Constant(1.0) .- y) .* log.(Constant(1.0) .- ŷ .+ ϵ)))\n",
    "\n",
    "wh = Variable(randn(hidden_neurons, input_neurons) * sqrt(2 / input_neurons), name=\"wh\")\n",
    "wo = Variable(randn(output_neurons, hidden_neurons), name=\"wo\")\n",
    "bh = Variable(zeros(hidden_neurons, 1), name=\"bh\")\n",
    "bo = Variable(zeros(output_neurons, 1), name=\"bo\")\n",
    "x = Variable(zeros(input_neurons), name=\"x\")\n",
    "y = Variable(zeros(output_neurons), name=\"y\")\n",
    "\n",
    "y = Variable(zeros(1, 64), name=\"y\")  # max batch size\n",
    "x = Variable(zeros(input_neurons, 64), name=\"x\")\n",
    "\n",
    "function dense(w, b, x, activation) return activation.(w * x .+ b) end\n",
    "\n",
    "function net(x, wh, bh, wo, bo)\n",
    "    x̂ = dense(wh, bh, x, relu)\n",
    "    x̂.name = \"x̂\"\n",
    "    ŷ = dense(wo, bo, x̂, σ)\n",
    "    ŷ.name = \"ŷ\"\n",
    "    return ŷ\n",
    "end\n",
    "\n",
    "function loss(x, y, wh, bh, wo, bo)\n",
    "    ŷ = net(x, wh, bh, wo, bo)\n",
    "    E = binary_cross_entropy_loss(y, ŷ); E.name = \"loss\"\n",
    "    return E, ŷ\n",
    "end\n",
    "\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "adam_state = Dict{Variable, Tuple{Array, Array, Int}}()\n",
    "for param in [wh, wo, bh, bo]\n",
    "    m = zeros(size(param.output))\n",
    "    v = zeros(size(param.output))\n",
    "    adam_state[param] = (m, v, 0)\n",
    "end\n",
    "\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (xb, yb) in dataset\n",
    "            batch_size = size(xb, 2)\n",
    "\n",
    "            x.output .= xb\n",
    "            y.output .= yb\n",
    "\n",
    "            L, ŷ_node = loss(x, y, wh, bh, wo, bo)\n",
    "\n",
    "            graph = topological_sort(L)\n",
    "\n",
    "            lval = forward!(graph)\n",
    "\n",
    "            for param in [wh, wo, bh, bo]\n",
    "                param.gradient = nothing\n",
    "            end\n",
    "            backward!(graph)\n",
    "\n",
    "            # ADAM\n",
    "            β1 = 0.9\n",
    "            β2 = 0.999\n",
    "            ε = 1e-8\n",
    "\n",
    "            for param in [wh, wo, bh, bo]\n",
    "                g = param.gradient\n",
    "                if size(g) != size(param.output)\n",
    "                    g = mean(g; dims=2)\n",
    "                    g = dropdims(g; dims=2)\n",
    "                end\n",
    "\n",
    "                m, v, t = adam_state[param]\n",
    "                t += 1\n",
    "\n",
    "                m .= β1 .* m .+ (1 - β1) .* g\n",
    "                v .= β2 .* v .+ (1 - β2) .* (g .^ 2)\n",
    "\n",
    "                m_hat = m ./ (1 - β1^t)\n",
    "                v_hat = v ./ (1 - β2^t)\n",
    "\n",
    "                param.output .-= lr .* m_hat ./ (sqrt.(v_hat) .+ ε)\n",
    "\n",
    "                adam_state[param] = (m, v, t)\n",
    "            end\n",
    "\n",
    "            ŷ = ŷ_node.output\n",
    "            predictions = ŷ .> 0.5\n",
    "            targets = y.output .> 0.5\n",
    "            total_correct += count(predictions .== targets)\n",
    "            total_loss += lval[1] * batch_size\n",
    "            total_samples += batch_size\n",
    "        end\n",
    "    end\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (loss: %.4f, acc: %.4f)\",\n",
    "                     epoch, t, avg_loss, avg_acc))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36ebdf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../autodiff/flux_like_api.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f686fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((Dense{typeof(relu)}(Variable(Float32[0.005698695 -0.003132612 … 0.015349576 0.016572604; 0.015555462 -0.013308286 … 0.007920059 0.011096501; … ; -0.015656058 -0.004709352 … -0.0025984442 0.008416636; -0.014263206 0.015561428 … -0.004636731 0.0023076062], nothing, \"weight\"), Variable([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], nothing, \"bias\"), Main.relu), Dense{typeof(σ)}(Variable(Float32[-0.19049272 -0.09341225 … -0.22478186 0.08881199], nothing, \"weight\"), Variable([0.0], nothing, \"bias\"), Main.σ)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(input_neurons, hidden_neurons, relu),\n",
    "    Dense(hidden_neurons, output_neurons, σ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc10bacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], nothing, \"x\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = Variable(zeros(1, 64), name=\"y\")  # max batch size\n",
    "x = Variable(zeros(input_neurons, 64), name=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28eecc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss(model, x, y)\n",
    "    ŷ = model(x)\n",
    "    E = binary_cross_entropy_loss(y, ŷ)\n",
    "    E.name = \"loss\"\n",
    "    return E, ŷ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb735402",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_state = Dict{Variable,Tuple{Array,Array,Int}}()\n",
    "for param in trainable(model)\n",
    "    m = zeros(size(param.output))\n",
    "    v = zeros(size(param.output))\n",
    "    adam_state[param] = (m, v, 0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e778d401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(adam_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa69c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (2.99s) \tTrain: (loss: 0.6494, acc: 0.7895)\n",
      "Epoch: 2 (3.20s) \tTrain: (loss: 0.4663, acc: 0.9114)\n",
      "Epoch: 3 (3.27s) \tTrain: (loss: 0.3023, acc: 0.9403)\n",
      "Epoch: 4 (3.15s) \tTrain: (loss: 0.2062, acc: 0.9626)\n",
      "Epoch: 5 (3.21s) \tTrain: (loss: 0.1475, acc: 0.9756)\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (xb, yb) in dataset\n",
    "            batch_size = size(xb, 2)\n",
    "\n",
    "            x.output .= xb\n",
    "            y.output .= yb\n",
    "\n",
    "            L, ŷ_node = loss(model, x, y)\n",
    "\n",
    "            # TODO: Sort the graph only once\n",
    "            graph = topological_sort(L)\n",
    "\n",
    "            lval = forward!(graph)\n",
    "\n",
    "            for param in trainable(model)\n",
    "                param.gradient = nothing\n",
    "            end\n",
    "            backward!(graph)\n",
    "\n",
    "            # ADAM\n",
    "            β1 = 0.9\n",
    "            β2 = 0.999\n",
    "            ε = 1e-8\n",
    "\n",
    "            for param in trainable(model)\n",
    "                g = param.gradient\n",
    "                if size(g) != size(param.output)\n",
    "                    g = mean(g; dims=2)\n",
    "                    g = dropdims(g; dims=2)\n",
    "                end\n",
    "\n",
    "                m, v, t = adam_state[param]\n",
    "                t += 1\n",
    "\n",
    "                m .= β1 .* m .+ (1 - β1) .* g\n",
    "                v .= β2 .* v .+ (1 - β2) .* (g .^ 2)\n",
    "\n",
    "                m_hat = m ./ (1 - β1^t)\n",
    "                v_hat = v ./ (1 - β2^t)\n",
    "\n",
    "                param.output .-= lr .* m_hat ./ (sqrt.(v_hat) .+ ε)\n",
    "\n",
    "                adam_state[param] = (m, v, t)\n",
    "            end\n",
    "\n",
    "            ŷ = ŷ_node.output\n",
    "            predictions = ŷ .> 0.5\n",
    "            targets = y.output .> 0.5\n",
    "            total_correct += count(predictions .== targets)\n",
    "            total_loss += lval[1] * batch_size\n",
    "            total_samples += batch_size\n",
    "        end\n",
    "    end\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (loss: %.4f, acc: %.4f)\",\n",
    "        epoch, t, avg_loss, avg_acc))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4583a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Variable, Tuple{Array, Array, Int64}} with 4 entries:\n",
       "  Variable(Float32[-0.1904… => ([0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0], 0)\n",
       "  Variable(Float32[0.00569… => ([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 …\n",
       "  Variable([0.0, 0.0, 0.0,… => ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n",
       "  Variable([0.0], nothing,… => ([0.0], [0.0], 0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = Adam()\n",
    "adam_state = setup(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ecfe748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (3.83s) \tTrain: (loss: 0.6453, acc: 0.7745)\n",
      "Epoch: 2 (3.20s) \tTrain: (loss: 0.4581, acc: 0.9136)\n",
      "Epoch: 3 (3.51s) \tTrain: (loss: 0.2980, acc: 0.9413)\n",
      "Epoch: 4 (3.30s) \tTrain: (loss: 0.2038, acc: 0.9629)\n",
      "Epoch: 5 (4.21s) \tTrain: (loss: 0.1459, acc: 0.9751)\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1:epochs\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        for (xb, yb) in dataset\n",
    "            batch_size = size(xb, 2)\n",
    "\n",
    "            x.output .= xb\n",
    "            y.output .= yb\n",
    "\n",
    "            L, ŷ_node = loss(model, x, y)\n",
    "\n",
    "            # TODO: Sort the graph only once\n",
    "            graph = topological_sort(L)\n",
    "\n",
    "            lval = forward!(graph)\n",
    "\n",
    "            for param in trainable(model)\n",
    "                param.gradient = nothing\n",
    "            end\n",
    "            backward!(graph)\n",
    "\n",
    "            update!(opt, adam_state, model)\n",
    "\n",
    "            ŷ = ŷ_node.output\n",
    "            predictions = ŷ .> 0.5\n",
    "            targets = y.output .> 0.5\n",
    "            total_correct += count(predictions .== targets)\n",
    "            total_loss += lval[1] * batch_size\n",
    "            total_samples += batch_size\n",
    "        end\n",
    "    end\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (loss: %.4f, acc: %.4f)\",\n",
    "        epoch, t, avg_loss, avg_acc))\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
